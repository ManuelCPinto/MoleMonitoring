{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10705909,"sourceType":"datasetVersion","datasetId":6635032}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport time\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom PIL import Image\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import label_binarize\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport cv2\nimport timm\n\n# ---------------------------\n# Dataset Class (global scope)\n# ---------------------------\n\nclass SkinLesionDataset(Dataset):\n    def __init__(self, df, folder, label_map, transform, oversample_targets=None, is_test=False):\n        self.label_map = label_map\n        self.folder = folder\n        self.transform = transform\n        self.is_test = is_test\n        self.df = df\n\n        if oversample_targets and not is_test:\n            print(\"Oversampling data for classes ...\")\n            grp = df.groupby(\"dx\")\n            balanced_df = []\n            for dx, group in grp:\n                target_num = oversample_targets.get(dx, len(group))\n                mult = max(1, target_num // len(group))\n                balanced_df.append(pd.DataFrame(np.repeat(group.values, mult, axis=0), columns=group.columns))\n            self.df = pd.concat(balanced_df).reset_index(drop=True)\n            print(f\"After oversampling, total samples: {len(self.df)}\")\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        lbl = self.label_map[row[\"dx\"]]\n        path = os.path.join(self.folder, f\"{row['image_id']}.jpg\")\n        img = np.array(Image.open(path).convert(\"RGB\"))\n        augmented = self.transform(image=img)\n        image_tensor = augmented[\"image\"].float()\n        return image_tensor, torch.tensor(lbl, dtype=torch.long), row[\"image_id\"]\n\n# ---------------------------\n# Soft Attention Module\n# ---------------------------\nclass SoftAttention(nn.Module):\n    def __init__(self, channels, heads, aggregate=True, concat_with_x=False):\n        super(SoftAttention, self).__init__()\n        self.channels = channels\n        self.multiheads = heads\n        self.aggregate_channels = aggregate\n        self.concat_input_with_scaled = concat_with_x\n        self.conv = nn.Conv3d(1, heads, kernel_size=(channels, 3, 3), padding=(0, 1, 1), bias=True)\n\n    def forward(self, x):\n        b, c, h, w = x.shape\n        x_exp = x.unsqueeze(1) \n        conv3d = self.conv(x_exp).squeeze(2) \n        attn_maps = F.softmax(conv3d.view(b, self.multiheads, -1), dim=-1).view(b, self.multiheads, h, w)\n        if self.aggregate_channels:\n            attn_maps = attn_maps.sum(dim=1, keepdim=True)\n            x_out = x * attn_maps\n        else:\n            x_out = x * attn_maps.unsqueeze(1)\n        if self.concat_input_with_scaled:\n            return torch.cat([x_out, x], dim=1)\n        return x_out\n\n# ---------------------------\n# Model Definition: InceptionResNetV2 with Soft Attention\n# ---------------------------\nclass InceptionResNetV2_SoftAttention(nn.Module):\n    def __init__(self, num_classes, dropout_p):\n        super(InceptionResNetV2_SoftAttention, self).__init__()\n        self.num_classes = num_classes \n        print(\"Creating base model (InceptionResNetV2)...\")\n        self.base_model = timm.create_model(\"inception_resnet_v2\", pretrained=True, num_classes=0, global_pool=\"\")\n        print(\"Base model created.\")\n        self.soft_attention = SoftAttention(1536, heads=16, aggregate=True)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=1)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(dropout_p)\n        self.fc = None \n\n    def forward(self, x):\n        features = self.base_model.forward_features(x) \n        attn_features = self.soft_attention(features)\n        pooled_features = self.pool(features)\n        pooled_attn = self.pool(attn_features)\n        combined = torch.cat([pooled_features, pooled_attn], dim=1)\n        activated = self.relu(combined)\n        dropped = self.dropout(activated)\n        flat = torch.flatten(dropped, 1)\n        if self.fc is None:\n            self.fc = nn.Linear(flat.shape[1], self.num_classes).to(flat.device)\n        out = self.fc(flat)\n        return out\n\n# ---------------------------\n# Evaluation Function\n# ---------------------------\ndef evaluate_dataset(model, loader, device, ham_classes, num_classes, title=\"Test Data\"):\n    y_true, y_pred, y_prob = [], [], []\n    model.eval()\n    with torch.no_grad():\n        for images, labels, _ in loader:\n            images = images.to(device)\n            labels = labels.to(device)\n            outputs = model(images)\n            probs = F.softmax(outputs, dim=1)\n            preds = outputs.argmax(dim=1)\n            y_true.extend(labels.cpu().numpy())\n            y_pred.extend(preds.cpu().numpy())\n            y_prob.extend(probs.cpu().numpy())\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    y_prob = np.array(y_prob)\n\n    print(f\"\\n**Classification Report on {title}:**\")\n    print(classification_report(y_true, y_pred, target_names=ham_classes))\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=ham_classes, yticklabels=ham_classes)\n    plt.title(f\"Confusion Matrix - {title}\")\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.show()\n\n    y_true_bin = label_binarize(y_true, classes=list(range(num_classes)))\n    for i in range(num_classes):\n        fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_prob[:, i])\n        roc_auc = auc(fpr, tpr)\n        plt.plot(fpr, tpr, label=f\"{ham_classes[i]} (AUC = {roc_auc:.2f})\")\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.title(f\"ROC Curves - {title}\")\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n# ---------------------------\n# Main Function\n# ---------------------------\ndef main():\n    print(\"Starting main function...\")\n\n    BASE_PATH = \"/kaggle/input/ham10000\"\n    \n    # HAM10000 paths\n    HAM_IMAGES_FOLDER = os.path.join(BASE_PATH, \"HAM10000\", \"HAM10000_images\")\n    HAM_METADATA_FILE = os.path.join(BASE_PATH, \"HAM10000\", \"HAM10000_metadata\")  # Ensure CSV\n\n    # ISIC2018 paths\n    ISIC_IMAGES_FOLDER = os.path.join(BASE_PATH, \"ISIC2018\", \"ISIC2018_images\")\n    ISIC_METADATA_FILE = os.path.join(BASE_PATH, \"ISIC2018\", \"ISIC2018_metadata\")  # Ensure CSV\n\n    CHECKPOINT_PATH = \"best_inception_resnetv2_attention.pth\"\n\n    # Hyperparameters\n    NUM_EPOCHS = 100\n    IMG_SIZE = 299\n    BATCH_SIZE = 64\n    LR = 1e-4\n    DROPOUT_P = 0.5\n    PATIENCE = 15\n\n    ham_classes = [\"nv\", \"mel\", \"bkl\", \"bcc\", \"akiec\", \"vasc\", \"df\"]\n    label_map = {name: idx for idx, name in enumerate(ham_classes)}\n    NUM_CLASSES = len(ham_classes)\n\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(\"Using device:\", device)\n    if device == \"cuda\":\n        torch.backends.cudnn.benchmark = True\n\n    # ---------------------------\n    # Augmentation Transforms\n    # ---------------------------\n    print(\"Setting up augmentation transforms...\")\n    train_transform = A.Compose([\n        A.RandomResizedCrop(size=(IMG_SIZE, IMG_SIZE), scale=(0.8, 1.0), ratio=(0.9, 1.1), interpolation=cv2.INTER_CUBIC, p=1.0),\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.5),\n        A.Rotate(limit=180, p=0.5),\n        A.Affine(scale=(0.9, 1.1), translate_percent=(-0.1, 0.1), rotate=(-30, 30), p=0.5),\n        A.RGBShift(r_shift_limit=20, g_shift_limit=20, b_shift_limit=20, p=0.5),\n        A.RandomBrightnessContrast(p=0.5),\n        A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n        ToTensorV2()\n    ])\n    test_transform = A.Compose([\n        A.Resize(height=IMG_SIZE, width=IMG_SIZE),\n        A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n        ToTensorV2()\n    ])\n    print(\"Transforms set up.\")\n\n    oversample_targets = {\n        \"mel\": 4000,\n        \"akiec\": 3500,\n        \"bcc\": 3000,\n        \"bkl\": 2600,\n        \"df\": 2000,\n        \"vasc\": 2000\n    }\n\n    # ---------------------------\n    # Data Loading and Splitting for HAM10000\n    # ---------------------------\n    print(\"Reading HAM metadata file...\")\n    ham_df = pd.read_csv(HAM_METADATA_FILE)\n    # Split HAM into 80% train, 10% validation, 10% test\n    train_df, temp_df = train_test_split(ham_df, test_size=0.2, stratify=ham_df[\"dx\"], random_state=42)\n    val_df, ham_test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df[\"dx\"], random_state=42)\n    print(f\"HAM Training samples: {len(train_df)}, Validation samples: {len(val_df)}, Test samples: {len(ham_test_df)}\")\n\n    # Create datasets for HAM splits\n    train_dataset = SkinLesionDataset(train_df, HAM_IMAGES_FOLDER, label_map, train_transform, oversample_targets)\n    val_dataset = SkinLesionDataset(val_df, HAM_IMAGES_FOLDER, label_map, test_transform, is_test=True)\n    ham_test_dataset = SkinLesionDataset(ham_test_df, HAM_IMAGES_FOLDER, label_map, test_transform, is_test=True)\n\n    # Create DataLoaders\n    num_workers = os.cpu_count() - 2 if os.cpu_count() and os.cpu_count() > 2 else 0\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n                              num_workers=num_workers, pin_memory=True,\n                              persistent_workers=True, prefetch_factor=2)\n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n                            num_workers=num_workers, pin_memory=True,\n                            persistent_workers=True, prefetch_factor=2)\n    ham_test_loader = DataLoader(ham_test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n                                 num_workers=num_workers, pin_memory=True,\n                                 persistent_workers=True, prefetch_factor=2)\n    print(\"HAM DataLoaders created.\")\n\n    # Load ISIC metadata and create test DataLoader for ISIC\n    print(\"Reading ISIC metadata file and creating DataLoader...\")\n    isic_df = pd.read_csv(ISIC_METADATA_FILE)\n    isic_test_dataset = SkinLesionDataset(isic_df, ISIC_IMAGES_FOLDER, label_map, test_transform, is_test=True)\n    isic_test_loader = DataLoader(isic_test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n                                  num_workers=num_workers, pin_memory=True,\n                                  persistent_workers=True, prefetch_factor=2)\n    print(\"ISIC Test DataLoader created.\")\n\n    # ---------------------------\n    # Model, Loss, Optimizer, and Scheduler Setup\n    # ---------------------------\n    print(\"Creating model...\")\n    model = InceptionResNetV2_SoftAttention(num_classes=NUM_CLASSES, dropout_p=DROPOUT_P).to(device)\n    print(\"Model created.\")\n    class_weights = torch.tensor([1.0, 1.0, 1.0, 1.0, 5.0, 1.0, 1.0]).to(device)\n    criterion = nn.CrossEntropyLoss(weight=class_weights)\n    optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=1e-4)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5, verbose=True)\n    scaler = torch.amp.GradScaler() if device == 'cuda' else None\n\n    best_val_acc = 0.0\n    patience_counter = 0\n\n    # ---------------------------\n    # Training Loop\n    # ---------------------------\n    print(\"Starting training loop...\")\n    for epoch in range(NUM_EPOCHS):\n        start_time = time.time()\n        model.train()\n        running_loss, running_correct, total = 0.0, 0, 0\n\n        for images, labels, _ in train_loader:\n            images = images.to(device, memory_format=torch.channels_last)\n            labels = labels.to(device)\n            optimizer.zero_grad()\n\n            with torch.amp.autocast(device_type=\"cuda\", enabled=(scaler is not None)):\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n\n            if scaler is not None:\n                scaler.scale(loss).backward()\n                scaler.step(optimizer)\n                scaler.update()\n            else:\n                loss.backward()\n                optimizer.step()\n\n            running_loss += loss.item() * images.size(0)\n            running_correct += (outputs.argmax(dim=1) == labels).sum().item()\n            total += labels.size(0)\n\n        train_loss = running_loss / total\n        train_acc  = running_correct / total\n\n        # Validation on HAM validation set\n        model.eval()\n        val_loss, val_correct, val_total = 0.0, 0, 0\n        with torch.no_grad(), torch.amp.autocast(device_type=\"cuda\", enabled=(scaler is not None)):\n            for images, labels, _ in val_loader:\n                images = images.to(device, memory_format=torch.channels_last)\n                labels = labels.to(device)\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item() * images.size(0)\n                val_correct += (outputs.argmax(dim=1) == labels).sum().item()\n                val_total += labels.size(0)\n\n        val_loss /= val_total\n        val_acc = val_correct / val_total\n        epoch_time = time.time() - start_time\n\n        scheduler.step(val_acc)\n\n        print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] -> Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Time: {epoch_time:.2f} sec\")\n\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            patience_counter = 0\n            torch.save(model.state_dict(), CHECKPOINT_PATH)\n            print(\"New best model saved!\")\n        else:\n            patience_counter += 1\n            if patience_counter >= PATIENCE:\n                print(\"Early stopping triggered!\")\n                break\n\n    print(\"Training finished. Loading best model...\")\n    model.load_state_dict(torch.load(CHECKPOINT_PATH))\n    model.eval()\n\n    # ---------------------------\n    # Evaluation on HAM Test Set\n    # ---------------------------\n    evaluate_dataset(model, ham_test_loader, device, ham_classes, NUM_CLASSES, title=\"HAM Test Set\")\n\n    # ---------------------------\n    # Evaluation on ISIC Test Set\n    # ---------------------------\n    evaluate_dataset(model, isic_test_loader, device, ham_classes, NUM_CLASSES, title=\"ISIC Test Set\")\n\nif __name__ == '__main__':\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-10T16:01:22.675460Z","iopub.execute_input":"2025-02-10T16:01:22.675957Z","iopub.status.idle":"2025-02-10T22:22:45.424580Z","shell.execute_reply.started":"2025-02-10T16:01:22.675915Z","shell.execute_reply":"2025-02-10T22:22:45.423404Z"}},"outputs":[],"execution_count":null}]}